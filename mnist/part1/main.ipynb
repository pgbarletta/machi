{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.12 64-bit ('6.86x': conda)",
   "metadata": {
    "interpreter": {
     "hash": "a35ab2152ec5bc05db4af2f0bd9e315137fe5ccba59505788c94054fdcf835b2"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "sys.path.append(\"..\")\n",
    "from utils import *\n",
    "from linear_regression import *\n",
    "from svm import *\n",
    "from softmax import *\n",
    "from features import *\n",
    "from kernel import *\n",
    "from sklearn.svm import LinearSVC\n",
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose = False\n",
    "\n",
    "epsilon = 1e-6\n",
    "\n",
    "def green(s):\n",
    "    return '\\033[1;32m%s\\033[m' % s\n",
    "\n",
    "def yellow(s):\n",
    "    return '\\033[1;33m%s\\033[m' % s\n",
    "\n",
    "def red(s):\n",
    "    return '\\033[1;31m%s\\033[m' % s\n",
    "\n",
    "def log(*m):\n",
    "    print(\" \".join(map(str, m)))\n",
    "\n",
    "def log_exit(*m):\n",
    "    log(red(\"ERROR:\"), *m)\n",
    "    exit(1)\n",
    "\n",
    "\n",
    "def check_real(ex_name, f, exp_res, *args):\n",
    "    try:\n",
    "        res = f(*args)\n",
    "    except NotImplementedError:\n",
    "        log(red(\"FAIL\"), ex_name, \": not implemented\")\n",
    "        return True\n",
    "    if not np.isreal(res):\n",
    "        log(red(\"FAIL\"), ex_name, \": does not return a real number, type: \", type(res))\n",
    "        return True\n",
    "    if not -epsilon < res - exp_res < epsilon:\n",
    "        log(red(\"FAIL\"), ex_name, \": incorrect answer. Expected\", exp_res, \", got: \", res)\n",
    "        return True\n",
    "\n",
    "\n",
    "def equals(x, y):\n",
    "    if type(y) == np.ndarray:\n",
    "        return (np.abs(x - y) < epsilon).all()\n",
    "    return -epsilon < x - y < epsilon\n",
    "\n",
    "def check_array(ex_name, f, exp_res, *args):\n",
    "    try:\n",
    "        res = f(*args)\n",
    "    except NotImplementedError:\n",
    "        log(red(\"FAIL\"), ex_name, \": not implemented\")\n",
    "        return True\n",
    "    if not type(res) == np.ndarray:\n",
    "        log(red(\"FAIL\"), ex_name, \": does not return a numpy array, type: \", type(res))\n",
    "        return True\n",
    "    if not len(res) == len(exp_res):\n",
    "        log(red(\"FAIL\"), ex_name, \": expected an array of shape \", exp_res.shape, \" but got array of shape\", res.shape)\n",
    "        return True\n",
    "    if not equals(res, exp_res):\n",
    "        log(red(\"FAIL\"), ex_name, \": incorrect answer. Expected\", exp_res, \", got: \", res)\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closed_form(X, Y, lambda_factor):\n",
    "    \"\"\"\n",
    "    Computes the closed form solution of linear regression with L2 regularization\n",
    "\n",
    "    Args:\n",
    "        X - (n, d + 1) NumPy array (n datapoints each with d features plus the bias feature in the first dimension)\n",
    "        Y - (n, ) NumPy array containing the labels (a number from 0-9) for each\n",
    "            data point\n",
    "        lambda_factor - the regularization constant (scalar)\n",
    "    Returns:\n",
    "        theta - (d + 1, ) NumPy array containing the weights of linear regression. Note that theta[0]\n",
    "        represents the y-axis intercept of the model and therefore X[0] = 1\n",
    "    \"\"\"\n",
    "\n",
    "    A = (X.T @ X) + (np.identity(X.shape[1]) * lambda_factor)\n",
    "    b = X.T @ Y\n",
    "\n",
    "    rtdo = np.linalg.inv(A) @ b\n",
    "    \n",
    "    return rtdo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[0.95892691, 0.29797309, 0.42508341]\n",
    ",[0.11065406, 0.73685701, 0.62536117]\n",
    ",[0.64200465, 0.03767413, 0.95782812]\n",
    ",[0.82838564, 0.58323297, 0.4352333]\n",
    ",[0.19839889, 0.80945848, 0.8674617]\n",
    ",[0.98415849, 0.57810422, 0.19369748]\n",
    ",[0.4715416, 0.96105139, 0.85578663]\n",
    ",[0.03546793, 0.6986972, 0.75711946]\n",
    ",[0.30809539, 0.86729695, 0.40138412]\n",
    ",[0.99220048, 0.43055934, 0.30102598]\n",
    ",[0.75688374, 0.97314883, 0.17028019]\n",
    ",[0.80568499, 0.97973042, 0.05511123]\n",
    ",[0.60451808, 0.10718601, 0.78651937]\n",
    ",[0.91556592, 0.90700619, 0.12362445]\n",
    ",[0.64209056, 0.05692244, 0.44575169]\n",
    ",[0.39288006, 0.47537458, 0.52895656]])\n",
    "\n",
    "Y = np.array([0.14923122, 0.49691812, 0.05387376, 0.86180237, 0.94807534, 0.25180968\n",
    ",0.12675531, 0.0356647, 0.33843285, 0.95558534, 0.23558956, 0.30578756\n",
    ",0.37154414, 0.81286067, 0.90505945, 0.60942559])\n",
    "\n",
    "lambda_factor = 0.5126229286392731"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "closed_form(X, Y, lambda_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_vs_rest_svm(train_x, train_y, test_x):\n",
    "    \"\"\"\n",
    "    Trains a linear SVM for binary classifciation\n",
    "\n",
    "    Args:\n",
    "        train_x - (n, d) NumPy array (n datapoints each with d features)\n",
    "        train_y - (n, ) NumPy array containing the labels (0 or 1) for each training data point\n",
    "        test_x - (m, d) NumPy array (m datapoints each with d features)\n",
    "    Returns:\n",
    "        pred_test_y - (m,) NumPy array containing the labels (0 or 1) for each test data point\n",
    "    \"\"\"\n",
    "    lsvc = LinearSVC(random_state = 0, C=0.1)\n",
    "\n",
    "    lsvc.fit(train_x, train_y)\n",
    "    \n",
    "    pred_test_y = lsvc.predict(test_x)\n",
    "    \n",
    "    return pred_test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_vs_rest_svm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_class_svm(train_x, train_y, test_x):\n",
    "    \"\"\"\n",
    "    Trains a linear SVM for multiclass classifciation using a one-vs-rest strategy\n",
    "\n",
    "    Args:\n",
    "        train_x - (n, d) NumPy array (n datapoints each with d features)\n",
    "        train_y - (n, ) NumPy array containing the labels (int) for each training data point\n",
    "        test_x - (m, d) NumPy array (m datapoints each with d features)\n",
    "    Returns:\n",
    "        pred_test_y - (m,) NumPy array containing the labels (int) for each test data point\n",
    "    \"\"\"\n",
    "\n",
    "    lsvc = LinearSVC(random_state = 0, C=0.1, multi_class= 'ovr')\n",
    "\n",
    "    lsvc.fit(train_x, train_y)\n",
    "    \n",
    "    pred_test_y = lsvc.predict(test_x)\n",
    "    \n",
    "    return pred_test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_class_svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_probabilities(X, theta, temp_parameter):\n",
    "    \"\"\"\n",
    "    Computes, for each datapoint X[i], the probability that X[i] is labeled as j\n",
    "    for j = 0, 1, ..., k-1\n",
    "\n",
    "    Args:\n",
    "        X - (n, d) NumPy array (n datapoints each with d features)\n",
    "        theta - (k, d) NumPy array, where row j represents the parameters of our model for label j\n",
    "        temp_parameter - the temperature parameter of softmax function (scalar)\n",
    "    Returns:\n",
    "        H - (k, n) NumPy array, where each entry H[j][i] is the probability that X[i] is labeled as j\n",
    "    \"\"\"\n",
    "    xt = (theta @ X.T) / temp_parameter\n",
    "    c = np.tile(np.max(xt, axis = 0), (xt.shape[0], 1))\n",
    "\n",
    "    xtc = xt - c\n",
    "\n",
    "    H = np.exp(xtc) / np.sum(np.exp(xtc), axis=0) \n",
    "\n",
    "    return H\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost_function(X, Y, theta, lambda_factor, temp_parameter):\n",
    "    \"\"\"\n",
    "    Computes the total cost over every datapoint.\n",
    "\n",
    "    Args:\n",
    "        X - (n, d) NumPy array (n datapoints each with d features)\n",
    "        Y - (n, ) NumPy array containing the labels (a number from 0-9) for each\n",
    "            data point\n",
    "        theta - (k, d) NumPy array, where row j represents the parameters of our\n",
    "                model for label j\n",
    "        lambda_factor - the regularization constant (scalar)\n",
    "        temp_parameter - the temperature parameter of softmax function (scalar)\n",
    "\n",
    "    Returns\n",
    "        c - the cost value (scalar)\n",
    "    \"\"\"\n",
    "    H = compute_probabilities(X, theta, temp_parameter)\n",
    "    regression_term = np.sum(np.log(np.max(H, axis = 0))) / H.shape[1]\n",
    "    regularization_term = np.sum(np.power(theta, 2)) * (lambda_factor / 2)\n",
    "    \n",
    "    return regularization_term - regression_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_name = \"Compute cost function\"\n",
    "n, d, k = 3, 5, 7\n",
    "X = np.arange(0, n * d).reshape(n, d)\n",
    "Y = np.arange(0, n)\n",
    "zeros = np.zeros((k, d))\n",
    "temp = 0.2\n",
    "lambda_factor = 0.5\n",
    "exp_res = 1.9459101490553135"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_gradient_descent_iteration(X, Y, theta, alpha, lambda_factor, temp_parameter):\n",
    "    \"\"\"\n",
    "    Runs one step of batch gradient descent\n",
    "\n",
    "    Args:\n",
    "        X - (n, d) NumPy array (n datapoints each with d features)\n",
    "        Y - (n, ) NumPy array containing the labels (a number from 0-9) for each\n",
    "            data point\n",
    "        theta - (k, d) NumPy array, where row j represents the parameters of our\n",
    "                model for label j\n",
    "        alpha - the learning rate (scalar)\n",
    "        lambda_factor - the regularization constant (scalar)\n",
    "        temp_parameter - the temperature parameter of softmax function (scalar)\n",
    "\n",
    "    Returns:\n",
    "        theta - (k, d) NumPy array that is the final value of parameters theta\n",
    "    \"\"\"\n",
    "    n = X.shape[0]\n",
    "    k = theta.shape[0]\n",
    "    d = theta.shape[1]\n",
    "\n",
    "    M = sparse.coo_matrix(([1]*n, (Y, range(n))), shape=(k,n)).toarray()\n",
    "    p = compute_probabilities(X, theta, temp_parameter)\n",
    "\n",
    "    termino_1 = ((M - p)  @ X) / (n * temp_parameter)\n",
    "    termino_2 = lambda_factor * theta\n",
    "    gdte = termino_2 - termino_1\n",
    "\n",
    "    return theta - (alpha * gdte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[ -7.14285714,  -5.23809524,  -3.33333333,  -1.42857143,\n",
       "          0.47619048],\n",
       "       [  9.52380952,  11.42857143,  13.33333333,  15.23809524,\n",
       "         17.14285714],\n",
       "       [ 26.19047619,  28.0952381 ,  30.        ,  31.9047619 ,\n",
       "         33.80952381],\n",
       "       [ -7.14285714,  -8.57142857, -10.        , -11.42857143,\n",
       "        -12.85714286],\n",
       "       [ -7.14285714,  -8.57142857, -10.        , -11.42857143,\n",
       "        -12.85714286],\n",
       "       [ -7.14285714,  -8.57142857, -10.        , -11.42857143,\n",
       "        -12.85714286],\n",
       "       [ -7.14285714,  -8.57142857, -10.        , -11.42857143,\n",
       "        -12.85714286]])"
      ]
     },
     "metadata": {},
     "execution_count": 368
    }
   ],
   "source": [
    "n, d, k = 3, 5, 7\n",
    "X = np.arange(0, n * d).reshape(n, d)\n",
    "Y = np.arange(0, n)\n",
    "zeros = np.zeros((k, d))\n",
    "alpha = 2\n",
    "temp_parameter = 0.2\n",
    "lambda_factor = 0.5\n",
    "\n",
    "run_gradient_descent_iteration(X, Y, zeros, alpha, lambda_factor, temp_parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[  3.5714286,   2.6190476,   1.6666667,   0.7142857,  -0.2380952],\n",
       "       [ -4.7619048,  -5.7142857,  -6.6666667,  -7.6190476,  -8.5714286],\n",
       "       [-13.0952381, -14.047619 , -15.       , -15.952381 , -16.9047619],\n",
       "       [  3.5714286,   4.2857143,   5.       ,   5.7142857,   6.4285714],\n",
       "       [  3.5714286,   4.2857143,   5.       ,   5.7142857,   6.4285714],\n",
       "       [  3.5714286,   4.2857143,   5.       ,   5.7142857,   6.4285714],\n",
       "       [  3.5714286,   4.2857143,   5.       ,   5.7142857,   6.4285714]])"
      ]
     },
     "metadata": {},
     "execution_count": 364
    }
   ],
   "source": [
    "def update_y(train_y, test_y):\n",
    "    \"\"\"\n",
    "    Changes the old digit labels for the training and test set for the new (mod 3)\n",
    "    labels.\n",
    "\n",
    "    Args:\n",
    "        train_y - (n, ) NumPy array containing the labels (a number between 0-9)\n",
    "                 for each datapoint in the training set\n",
    "        test_y - (n, ) NumPy array containing the labels (a number between 0-9)\n",
    "                for each datapoint in the test set\n",
    "\n",
    "    Returns:\n",
    "        train_y_mod3 - (n, ) NumPy array containing the new labels (a number between 0-2)\n",
    "                     for each datapoint in the training set\n",
    "        test_y_mod3 - (n, ) NumPy array containing the new labels (a number between 0-2)\n",
    "                    for each datapoint in the test set\n",
    "    \"\"\"\n",
    "    return (train_y % 3, test_y % 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = np.arange(0, 10)\n",
    "test_y = np.arange(9, -1, -1)\n",
    "exp_res = (\n",
    "        np.array([0, 1, 2, 0, 1, 2, 0, 1, 2, 0]),\n",
    "        np.array([0, 2, 1, 0, 2, 1, 0, 2, 1, 0])\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classification(X, theta, temp_parameter):\n",
    "    \"\"\"\n",
    "    Makes predictions by classifying a given dataset\n",
    "\n",
    "    Args:\n",
    "        X - (n, d - 1) NumPy array (n data points, each with d - 1 features)\n",
    "        theta - (k, d) NumPy array where row j represents the parameters of our model for\n",
    "                label j\n",
    "        temp_parameter - the temperature parameter of softmax function (scalar)\n",
    "\n",
    "    Returns:\n",
    "        Y - (n, ) NumPy array, containing the predicted label (a number between 0-9) for\n",
    "            each data point\n",
    "    \"\"\"\n",
    "    X = augment_feature_vector(X)\n",
    "    probabilities = compute_probabilities(X, theta, temp_parameter)\n",
    "    return np.argmax(probabilities, axis = 0)\n",
    "\n",
    "def plot_cost_function_over_time(cost_function_history):\n",
    "    plt.plot(range(len(cost_function_history)), cost_function_history)\n",
    "    plt.ylabel('Cost Function')\n",
    "    plt.xlabel('Iteration number')\n",
    "    plt.show()\n",
    "\n",
    "def compute_test_error(X, Y, theta, temp_parameter):\n",
    "    error_count = 0.\n",
    "    assigned_labels = get_classification(X, theta, temp_parameter)\n",
    "    return 1 - np.mean(assigned_labels == Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0, 1, 2, 0, 1, 2, 0, 1, 2, 0])"
      ]
     },
     "metadata": {},
     "execution_count": 371
    }
   ],
   "source": [
    "def compute_test_error_mod3(X, Y, theta, temp_parameter):\n",
    "    \"\"\"\n",
    "    Returns the error of these new labels when the classifier predicts the digit. (mod 3)\n",
    "\n",
    "    Args:\n",
    "        X - (n, d - 1) NumPy array (n datapoints each with d - 1 features)\n",
    "        Y - (n, ) NumPy array containing the labels (a number from 0-2) for each\n",
    "            data point\n",
    "        theta - (k, d) NumPy array, where row j represents the parameters of our\n",
    "                model for label j\n",
    "        temp_parameter - the temperature parameter of softmax function (scalar)\n",
    "\n",
    "    Returns:\n",
    "        test_error - the error rate of the classifier (scalar)\n",
    "    \"\"\"\n",
    "    \n",
    "    y_own = get_classification(X, theta, temp_parameter) % 3\n",
    "    error = 0\n",
    "    for x, b in zip(Y, y_own):\n",
    "        error += (x != b)\n",
    "\n",
    "    return (error / len(Y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_own = get_classification(X[:, 0:-1], theta, temp_parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "def center_data(X):\n",
    "    \"\"\"\n",
    "    Returns a centered version of the data, where each feature now has mean = 0\n",
    "\n",
    "    Args:\n",
    "        X - n x d NumPy array of n data points, each with d features\n",
    "\n",
    "    Returns:\n",
    "        - (n, d) NumPy array X' where for each i = 1, ..., n and j = 1, ..., d:\n",
    "        X'[i][j] = X[i][j] - means[j]       \n",
    "    - (d, ) NumPy array with the columns means\n",
    "\n",
    "    \"\"\"\n",
    "    feature_means = X.mean(axis=0)\n",
    "    return (X - feature_means), feature_means\n",
    "\n",
    "\n",
    "def principal_components(centered_data):\n",
    "    \"\"\"\n",
    "    Returns the principal component vectors of the data, sorted in decreasing order\n",
    "    of eigenvalue magnitude. This function first calculates the covariance matrix\n",
    "    and then finds its eigenvectors.\n",
    "\n",
    "    Args:\n",
    "        centered_data - n x d NumPy array of n data points, each with d features\n",
    "\n",
    "    Returns:\n",
    "        d x d NumPy array whose columns are the principal component directions sorted\n",
    "        in descending order by the amount of variation each direction (these are\n",
    "        equivalent to the d eigenvectors of the covariance matrix sorted in descending\n",
    "        order of eigenvalues, so the first column corresponds to the eigenvector with\n",
    "        the largest eigenvalue\n",
    "    \"\"\"\n",
    "    scatter_matrix = np.dot(centered_data.transpose(), centered_data)\n",
    "    eigen_values, eigen_vectors = np.linalg.eig(scatter_matrix)\n",
    "    # Re-order eigenvectors by eigenvalue magnitude:\n",
    "    idx = eigen_values.argsort()[::-1]\n",
    "    eigen_values = eigen_values[idx]\n",
    "    eigen_vectors = eigen_vectors[:, idx]\n",
    "    return eigen_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "def project_onto_PC(X, pcs, n_components, feature_means):\n",
    "    \"\"\"\n",
    "    Given principal component vectors pcs = principal_components(X)\n",
    "    this function returns a new data array in which each sample in X\n",
    "    has been projected onto the first n_components principcal components.\n",
    "    \"\"\"\n",
    "    # TODO: first center data using the feature_means\n",
    "    # TODO: Return the projection of the centered dataset\n",
    "    #       on the first n_components principal components.\n",
    "    #       This should be an array with dimensions: n x n_components.\n",
    "    # Hint: these principal components = first n_components columns\n",
    "    #       of the eigenvectors returned by principal_components().\n",
    "    #       Note that each eigenvector is already be a unit-vector,\n",
    "    #       so the projection may be done using matrix multiplication.\n",
    "    \n",
    "    x_cent = np.ndarray(X.shape)\n",
    "    for i in range(0, len(feature_means)):\n",
    "        x_cent[:, i] = X[:, i] - feature_means[i]\n",
    "\n",
    "    proj = x_cent @ pcs[:, 0:n_components]\n",
    "\n",
    "    return proj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([\n",
    "    [1, 2, 3],\n",
    "    [2, 4, 6],\n",
    "    [3, 6, 9],\n",
    "    [4, 8, 12],\n",
    "]);\n",
    "x_centered, feature_means = center_data(X)\n",
    "pcs = principal_components(x_centered)\n",
    "exp_res = np.array([\n",
    "    [5.61248608, 0],\n",
    "    [1.87082869, 0],\n",
    "    [-1.87082869, 0],\n",
    "    [-5.61248608, 0],\n",
    "])\n",
    "n_components = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[-2.67261242e-01, -8.99989016e-01, -2.59226735e-16],\n",
       "       [-5.34522484e-01, -1.58890294e-01, -8.32050294e-01],\n",
       "       [-8.01783726e-01,  4.05923201e-01,  5.54700196e-01]])"
      ]
     },
     "metadata": {},
     "execution_count": 427
    }
   ],
   "source": [
    "project_onto_PC(X, pcs, n_components, feature_means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cubic_features(X):\n",
    "    \"\"\"\n",
    "    Returns a new dataset with features given by the mapping\n",
    "    which corresponds to the cubic kernel.\n",
    "    \"\"\"\n",
    "    n, d = X.shape  # dataset size, input dimension\n",
    "    X_withones = np.ones((n, d + 1))\n",
    "    X_withones[:, :-1] = X\n",
    "    new_d = 0  # dimension of output\n",
    "    new_d = int((d + 1) * (d + 2) * (d + 3) / 6)\n",
    "\n",
    "    new_data = np.zeros((n, new_d))\n",
    "    col_index = 0\n",
    "    for x_i in range(n):\n",
    "        X_i = X[x_i]\n",
    "        X_i = X_i.reshape(1, X_i.size)\n",
    "\n",
    "        if d > 2:\n",
    "            comb_2 = np.matmul(np.transpose(X_i), X_i)\n",
    "\n",
    "            unique_2 = comb_2[np.triu_indices(d, 1)]\n",
    "            unique_2 = unique_2.reshape(unique_2.size, 1)\n",
    "            comb_3 = np.matmul(unique_2, X_i)\n",
    "            keep_m = np.zeros(comb_3.shape)\n",
    "            index = 0\n",
    "            for i in range(d - 1):\n",
    "                keep_m[index + np.arange(d - 1 - i), i] = 0\n",
    "\n",
    "                tri_keep = np.triu_indices(d - 1 - i, 1)\n",
    "\n",
    "                correct_0 = tri_keep[0] + index\n",
    "                correct_1 = tri_keep[1] + i + 1\n",
    "\n",
    "                keep_m[correct_0, correct_1] = 1\n",
    "                index += d - 1 - i\n",
    "\n",
    "            unique_3 = np.sqrt(6) * comb_3[np.nonzero(keep_m)]\n",
    "\n",
    "            new_data[x_i, np.arange(unique_3.size)] = unique_3\n",
    "            col_index = unique_3.size\n",
    "\n",
    "    for i in range(n):\n",
    "        newdata_colindex = col_index\n",
    "        for j in range(d + 1):\n",
    "            new_data[i, newdata_colindex] = X_withones[i, j]**3\n",
    "            newdata_colindex += 1\n",
    "            for k in range(j + 1, d + 1):\n",
    "                new_data[i, newdata_colindex] = X_withones[i, j]**2 * X_withones[i, k] * (3**(0.5))\n",
    "                newdata_colindex += 1\n",
    "\n",
    "                new_data[i, newdata_colindex] = X_withones[i, j] * X_withones[i, k]**2 * (3**(0.5))\n",
    "                newdata_colindex += 1\n",
    "\n",
    "                if k < d:\n",
    "                    new_data[i, newdata_colindex] = X_withones[i, j] * X_withones[i, k] * (6**(0.5))\n",
    "                    newdata_colindex += 1\n",
    "\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[1, 0], [0, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[1.        , 0.        , 0.        , 0.        , 1.73205081,\n",
       "        1.73205081, 0.        , 0.        , 0.        , 1.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 1.        , 1.73205081, 1.73205081, 1.        ]])"
      ]
     },
     "metadata": {},
     "execution_count": 443
    }
   ],
   "source": [
    "cubic_features(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial_kernel(X, Y, c, p):\n",
    "    \"\"\"\n",
    "        Compute the polynomial kernel between two matrices X and Y::\n",
    "            K(x, y) = (<x, y> + c)^p\n",
    "        for each pair of rows x in X and y in Y.\n",
    "\n",
    "        Args:\n",
    "            X - (n, d) NumPy array (n datapoints each with d features)\n",
    "            Y - (m, d) NumPy array (m datapoints each with d features)\n",
    "            c - a coefficient to trade off high-order and low-order terms (scalar)\n",
    "            p - the degree of the polynomial kernel\n",
    "\n",
    "        Returns:\n",
    "            kernel_matrix - (n, m) Numpy array containing the kernel matrix\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}